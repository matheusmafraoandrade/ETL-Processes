# -*- coding: utf-8 -*-
"""api_forms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uvp6pm3UilouWIfDhkhcKxfUkZosl3-P

### Pacotes necess치rios (todos os dias)
"""

# Commented out IPython magic to ensure Python compatibility.
#%%capture
#pip install oauth2client
#pip install PyOpenSSL
#pip install gspread
#pip install pydrive

import io

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from oauth2client.service_account import ServiceAccountCredentials
#from google.colab import files

file_path = {
  "type": "service_account",
  "project_id": "PROJECT_ID",
  "private_key_id": "PRIVATE_KEY_ID",
  "private_key": "PRIVATE_KEY",
  "client_email": "CLIENT_EMAIL",
  "client_id": "CLIENT_ID",
  "auth_uri": "AUTH_URI",
  "token_uri": "TOKEN_URI",
  "auth_provider_x509_cert_url": "PROVIDER_CERT_URL",
  "client_x509_cert_url": "CLIENT_CERT_URL"
}

scope = ['https://spreadsheets.google.com/feeds',
         'https://www.googleapis.com/auth/drive']

credentials = ServiceAccountCredentials.from_json_keyfile_dict(file_path, scope)

# to handle data retrieval
import urllib3
from urllib3 import request

# to handle certificate verification
import certifi

# to manage json data
import json

# for pandas dataframes
import pandas as pd

# to handle Google integration
import gspread

"""### All forms (everyday)"""

# handle certificate verification and SSL warnings
# https://urllib3.readthedocs.io/en/latest/user-guide.html#ssl
http = urllib3.PoolManager(
    cert_reqs='CERT_REQUIRED',
    ca_certs=certifi.where())

# https://legacydocs.hubspot.com/docs/methods/forms/v2/get_forms

# get data from the API; replace url with target source
url_forms = 'https://api.hubapi.com/forms/v2/forms?hapikey=API_KEY&formTypes=ALL'
r_forms = http.request('GET', url_forms)

# decode json data into a dict object
data_forms = json.loads(r_forms.data.decode('utf-8'))

# normalize the data dict and read it into a dataframe
# in this dataset, the data to extract is under 'features'
forms = pd.json_normalize(data_forms)[['guid', 'name']]
forms['name'] = forms['name'].str.replace('Collected form: ', '')
forms['name'] = forms['name'].apply(lambda x: x.strip())
#forms

"""### data extraction by form (everyday))"""

# https://legacydocs.hubspot.com/docs/methods/forms/get-submissions-for-a-form
# Last run: 58s

forms_list = list(forms.guid) # all forms ids
appended_df = [] # all forms dfs
success = [] # all forms with correct responses
failure = [] # all forms with errors in responses

for i in forms_list:
  try:
    # get data from the API; replace url with target source
    url = f'https://api.hubapi.com/form-integrations/v1/submissions/forms/{i}?hapikey=API_KEY&limit=50'
    r = http.request('GET', url)

    # decode json data into a dict object
    raw_data = json.loads(r.data.decode('utf-8'))

    # normalize the data dict and read it into a dataframe
    # in this dataset, the data to extract is under 'results'
    df = pd.json_normalize(raw_data, 'results')

    # transform data
    df_norm = pd.concat([pd.DataFrame(x) for x in df['values']], keys=df['submittedAt']
                        ).reset_index(level=1, drop=True).reset_index().drop(columns=['objectTypeId'])
    data = df_norm.pivot(values='value',index='submittedAt',columns='name').reset_index()
    data.columns.name=None

    # add forms columns
    data['guid'] = i
    data = data.merge(forms, on='guid', how='left')

    # append first page df to dfs list
    appended_df.append(data)

    # create and feed a list to iterate over submissions pages
    paging_ids = []
    paging_ids.append(raw_data['paging']['next']['after'])

    for p in paging_ids:
      try:
        # get data from the API; replace url with target source
        url = f'https://api.hubapi.com/form-integrations/v1/submissions/forms/{i}?hapikey=API_KEY&after={p}&limit=50'
        r = http.request('GET', url)

        # decode json data into a dict object
        raw_data = json.loads(r.data.decode('utf-8'))

        # normalize the data dict and read it into a dataframe
        # in this dataset, the data to extract is under 'results'
        df = pd.json_normalize(raw_data, 'results')

        # transform data
        df_norm = pd.concat([pd.DataFrame(x) for x in df['values']], keys=df['submittedAt']
                            ).reset_index(level=1, drop=True).reset_index().drop(columns=['objectTypeId'])
        data = df_norm.pivot(values='value',index='submittedAt',columns='name').reset_index()
        data.columns.name=None

        # add form informations columns
        data['guid'] = i
        data = data.merge(forms, on='guid', how='left')

        # add all other pages dfs to dfs list
        appended_df.append(data)

        # add next paging id to list
        paging_ids.append(raw_data['paging']['next']['after'])

      except:
        # this block runs when the last submissions page is reached
        continue

    # add successful form to list and delete paging ids to create another one
    success.append(i)
    del(paging_ids)
    
  except:
    # add form with errors to list and restart the loop
    failure.append(i)
    continue  

# concatenate all forms
appended_forms = pd.concat(appended_df)
print("Done: ", len(appended_forms))
print("Success: ", len(success))
print("Failure: ", len(failure))

"""### Transform data to spreadsheet format (everyday)"""

appended_forms.rename(columns={'submittedAt':'date_unix','name':'formul치rio'}, inplace=True)
appended_forms['date'] = pd.to_datetime(appended_forms['date_unix'], unit='ms').dt.date.astype('str')
df = appended_forms[['date','email','guid','formul치rio']].sort_values('formul치rio')
#df

"""### Load into Google Sheets (everyday)"""

#https://www.youtube.com/watch?v=cN7W2EPM-dw
#https://docs.gspread.org/en/latest/user-guide.html#using-gspread-with-pandas

gc = gspread.authorize(credentials)
wb = gc.open_by_key('SHEET_KEY') # Planilha de KPIs
ws = wb.worksheet('Envios de forms')

ws.update([df.columns.values.tolist()] + df.values.tolist())
